---
title: "Simulation Results"
author: "David Warton and Victor Tsang"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  github_document:
    toc: true
    fig_width: 5
    fig_height: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# TL;DR

* MINMI point estimates aren't as accurate as other methods (MLE) in high measurement error variation scenarios, because the minimum is not sufficient in the measurement error setting
* MINMI point estimates appear to be more biased in high measurement error settings and also more variable.
* As expected, MLE_INV is much slower than MINMI and asymptotic MLE approaches.
* Asymptotic MLE approaches have poor coverage probability when sample size and measurement error are both small, and as expected, MINMI and MLE_INV do fine on coverage.
* MINMI has wide confidence intervals when there is measurement error (because of inefficiency using minimum as statistic)


---

#### Load in the results

```{r, warning=FALSE, message=FALSE}
library(knitr)
library(tidyverse)
library(scales)
library(ggrepel)
library(gridExtra)
library(latex2exp)


load("data/synthetic-data-6-20230901.RData")
attach(synthetic.data.config)

RESULTS_PATH <- 'data/simResults-6-20230926.RData'
load(RESULTS_PATH)

head(results)
```

```{r}
results %>%
  group_by(method, error_factor) %>%
  summarise(point.pct_na = mean(point,na.rm=TRUE),
            lower.pct_na = mean(lower,na.rm=TRUE),
            upper.pct_na = mean(upper,na.rm=TRUE))
```

# Point Estimates

#### Calculate Metrics

```{r}
performance.point <- results %>%
  filter(!is.na(point),method%in%c("BA-MLE","Strauss","UNci","UT4ci","UTci","UTbias")) %>%
  group_by(error_factor, method) %>%
  summarise(MSE_000 = mean((point - theta.true)^2,na.rm=TRUE)/1000,
            bias = mean(point,na.rm=TRUE)-theta.true,
            variance_000 = var(point,na.rm=TRUE)/1000,
            avg_runtime = round(mean(point_runtime,na.rm=TRUE), 5))
```

```{r}
performance.point.tbl = vector(mode = "list", length(error_factors))

for (i in 1:length(error_factors)) {
  performance.point.tbl[[i]] <- performance.point %>%
    filter(error_factor == error_factors[i]) %>%
    ungroup() %>%
    mutate(across(!c(error_factor, method, avg_runtime), round)) %>%
    mutate(avg_runtime = round(avg_runtime, digits = 5)) %>%
    arrange(MSE_000)
}

performance.point.tbl[[1]]
performance.point.tbl[[2]]
performance.point.tbl[[3]]
performance.point.tbl[[4]]
performance.point.tbl[[5]]

```

## Show point estimation metrics for key methods
```{r}
performance.point%>%filter(method=="BA-MLE")
performance.point%>%filter(method=="UTci")
performance.point%>%filter(method=="UTbias")
```

#### Pivot to make plots

```{r}
performance.point.long <- performance.point %>%
  rename(Error = error_factor, Method = method, Bias = bias, Var_000 = variance_000, Runtime = avg_runtime) %>%
  pivot_longer(cols=c(MSE_000, Bias, Var_000, Runtime), names_to = "Metric")
  
performance.point.long
```

## Plots

```{r}
metrics = unique(performance.point.long$Metric)
performance.point_estimates.plots = lapply(metrics,
  function(met) {
    p = ggplot(data = filter(performance.point.long, Metric == met),
               mapping = aes(x = Error, y = value, colour = reorder(Method, value, decreasing=T))) +
      geom_line() +
      geom_point() +
      theme_bw() +
      labs(title = paste(met, "by Error"), ylab=NULL, colour = "Method") +
      theme(rect = element_rect(fill = "transparent")) +
      scale_color_manual(values = c("UNci" = "blue",
                                    "U0ci" = "forestgreen",
                                    "UTbias" = "plum",
                                    "UTci" = "purple"
                                    ))
    
    if (met %in% c("MSE", "Runtime")) {
      p = p+scale_y_log10(labels = label_comma())
    }
    p
  }
)

performance.point_estimates.plots[[1]] = performance.point_estimates.plots[[1]] + ylab("000's")
performance.point_estimates.plots[[2]] = performance.point_estimates.plots[[2]] + ylab("Years")
performance.point_estimates.plots[[3]] = performance.point_estimates.plots[[3]] + ylab("000's")
performance.point_estimates.plots[[4]] = performance.point_estimates.plots[[4]] + ylab("Seconds")

performance.point_estimates.plots[[1]]
performance.point_estimates.plots[[2]]
performance.point_estimates.plots[[3]]
performance.point_estimates.plots[[4]]
```


## Commentary

1. MSE:
    1. MINMI generally produces estimates with similar MSE to the MLE
    2. MINMI had the worst MSE in $4\sigma$ scenarios and was moderately bad in the $0\sigma$ scenario
2. Bias:
    1. MINMI does OK but has poor bias in the $4\sigma$ scenario, when minimum is clearly not the optimal statistic.
3. Variance:
    1. MINMI estimates generally have more variance than the other methods, especially in high measurement error scenarios.
4. Runtime:
    1. MINMI is comparable to MLE

```{r fig.width=15}
performance.point_estimates.plot.grid = do.call(grid.arrange, performance.point_estimates.plots)
performance.point_estimates.plot.grid
```


# Confidence Intervals

#### Calculate Metrics and Pivot

```{r}
performance.CI <- results %>%
  filter(!is.na(conf_int_runtime),method%in%c("UTwald","UT4ci","UTci","reginvU0","reginv","reginvUT")) %>%
  mutate(width = upper - lower,
         contains_theta = ifelse(theta.true > lower & theta.true < upper, 1, 0)) %>%
  group_by(error_factor, method) %>%
  summarise(Coverage = round(mean(contains_theta, na.rm=TRUE) * 100, 1),
            `Average Width` = round(mean(width, na.rm=TRUE), 2),
            `Trim Width` = mean(width, trim=0.05, na.rm=TRUE),
            `Average Runtime` = round(mean(conf_int_runtime, na.rm=TRUE), 5)) %>%
  ungroup() %>%
  arrange(method, error_factor)

performance.CI.long <- performance.CI %>%
  rename(Error = error_factor, Method = method, Width = `Average Width`, Runtime = `Average Runtime`) %>%
  pivot_longer(cols=c(Coverage, Width, Runtime),
               names_to = "Metric")
  
performance.CI.long
```

## Show CI metrics for key methods
```{r}
performance.CI%>%filter(method=="UT4ci")
performance.CI%>%filter(method%in%c("UTci","reginvUT"))
```

## Coverage Probability

```{r}
conf_int.coverage.plot <- performance.CI.long %>%
  filter(Metric == "Coverage") %>%
  ggplot(aes(x=Error, y=value, colour=reorder(Method, value, decreasing=T))) +
  geom_point() +
  geom_line(linewidth=0.5) +
  geom_label_repel(aes(label = value)) +
  theme_bw() +
  ylim(60,100) +
  labs(y = "Years", colour="Method", title="Coverage Probabilities") +
  scale_y_continuous(breaks=c(0, 25, 50, 75, 95, 100)) +
  theme(rect = element_rect(fill = "transparent")) +
      scale_color_manual(values = c("UTci" = "purple",
                                    "UTwald" = "pink",
                                    "reginv" = "blue",
                                    "reginvUT" = "plum",
                                    "reginvU0" = "forestgreen"))
conf_int.coverage.plot
```

## Widths

```{r}
conf_int.width.plot <- performance.CI.long %>%
  filter(Metric == "Width") %>%
  ggplot(aes(x=Error, y=value, colour=reorder(Method, value, decreasing=T))) +
  geom_point() +
  geom_line(linewidth=0.5) +
  theme_bw() +
  labs(y = "Years", colour="Method", title="Average Width of Estimated Confidence Intervals") +
  theme(rect = element_rect(fill = "transparent")) +
      scale_color_manual(values = c("UTci" = "purple",
                                    "UTwald" = "pink",
                                    "reginv" = "blue",
                                    "reginvUT" = "plum",
                                    "reginvU0" = "forestgreen"))
conf_int.width.plot
```

## Runtime

```{r}
conf_int.runtime.plot <- performance.CI.long %>%
  filter(Metric == "Runtime") %>%
  ggplot(aes(x=Error, y=value, colour=reorder(Method, value, decreasing=T))) +
  geom_point() +
  geom_line(linewidth=0.5) +
  theme_bw() +
  scale_y_continuous(trans=scales::log10_trans()) +
  labs(y = "Seconds", colour="Method", title="Average Runtime of Confidence Interval Estimation") +
  theme(rect = element_rect(fill = "transparent")) +
      scale_color_manual(values = c("UTci" = "purple",
                                    "UTwald" = "pink",
                                    "reginv" = "blue",
                                    "reginvUT" = "plum",
                                    "reginvU0" = "forestgreen"))
conf_int.runtime.plot
```

## Commentary

1. Coverage Probability:
    1. MINMI and MLE_INV have good coverage as expected
    2. Asymptotic MLE methods have poor coverage for small n and small measurement error, especially Wald
2. Confidence Interval Widths:
    1. MINMI has consistently wider CI's
    2. MLE_INV is also a bit wider, expected since other methods have undercoverage, although diff bigger than expected
3. Runtime
    1. MLE_INV much slower than everything else, and surprisingly, asymptotic MLE methods faster than MINMI.
  
#### Bonus: measurement error variation relative to our sampling error variation?

```{r}
pct_sigma_sampling <- 4*fossil.sd / (K-theta.true)

tibble(index = 1:n.samples, pct_sigma_sampling) %>%
  mutate(label = ifelse(pct_sigma_sampling > 0.3, percent(pct_sigma_sampling), "")) %>%
  ggplot(aes(x=index, y=pct_sigma_sampling)) +
  geom_point() +
  geom_label_repel(aes(label=label)) + 
  labs(x = 'Sample Index', y = '% of K - theta', title="Measurement Error Variation Relative to Sample Error Variation", subtitle = "(4sigma scenario)") +
  scale_y_continuous(labels = percent_format())
```

```{r}
tibble(index = 1:n.samples, pct_sigma_sampling) %>%
  ggplot(aes(x=pct_sigma_sampling)) +
  geom_histogram(binwidth=0.05) +
  scale_x_continuous(labels = percent_format())
  
```

Under $4\sigma$ scenario, we have a right skewed distribution. Our fossils are mostly <30% of $K-\theta$,  but we do get some samples with super large measurement error variation. Perhaps these cause problems?

#### Extra bonus: is the sampling distribution of MLE Gaussian for $\sigma>0$?

```{r distTheta}
errors=sort(unique(results$error_factor))
nError=length(errors)
par(mfrow=c(5,3),mgp=c(1.75,0.75,0),mar=c(3,2,0,0),oma=c(0,2,2,0))
for(iError in 1:nError)
{
  tmp=results%>%filter(method=="UTci" & error_factor==errors[iError]) %>% select(lower,point,upper)
  hist(tmp$lower,xlab="theta_lower",ylab="",main="")
  mtext(paste0("error_fac=",errors[iError]),2,line=2,font=2,cex=0.8)
  if(iError==1)
    mtext("theta_lower",3,font=2,cex=0.8)
  hist(tmp$point,xlab="theta_hat",ylab="",main="")
  if(iError==1)
    mtext("theta_hat",3,font=2)
  hist(tmp$point,xlab="theta_upper",ylab="",main="")
  if(iError==1)
    mtext("theta_upper",3,font=2)
}
```

Um, yes! Not at `r error_factor=0`, as expected, because this is a sample minimum.

Quantiles also seem to be approx normal with no outliers (except at `r error_factor`=4 where it looks like there is some non-convergence).

#### How good are standard error estimates?

```{r waldSE}
errors=unique(results$error_factor)
nError=length(errors)
par(mfrow=c(3,2),mgp=c(1.75,0.75,0),mar=c(3,2,0,0),oma=c(0,2,2,0))
for(iError in 1:nError)
{
  tmp=results%>%filter(method=="UTwald" & error_factor==errors[iError]) %>% select(point,point_runtime)
  nError=length(errors)
  hist(tmp$point_runtime,xlab="SE",ylab="",main="")
  abline(v=sd(tmp$point),col="red")
}
```