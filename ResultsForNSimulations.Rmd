---
title: "Simulation Results"
author: "Victor Tsang and David Warton"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  github_document:
    toc: true
    fig_width: 5
    fig_height: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# TL;DR

* MINMI point estimates aren't as accurate as MLE under Uniform-Normal (UN) model
* MINMI point estimates appear to be more biased than MLE
* main issue is that MINMI has much higher sample variance - and not decreasing with sample size!!
* The issue here is that sample minimum is not a good statistic for measurement error scenarios :(
* Coverage probability of MINMI and MLE_INV is good, asymptotic MLE is good except at low sample size and low measurement error


#### Which value of `error_factor` do we want plots for?
```{r whichError}
error_fac_to_plot = 1 #change this value to look at plots for a different error_factor
```

---

#### Load in the results

```{r allRes, warning=FALSE, message=FALSE}
library(knitr)
library(tidyverse)
library(scales)
library(ggrepel)
library(gridExtra)
library(latex2exp)


load("data/synthetic-data-12-20230808.RData")
attach(synthetic.data.config)

RESULTS_PATH <- 'data/simResults-12-20230821.RData'
load(RESULTS_PATH)

head(results)
all_results=results
all_results$n.samples = synthetic.data.config$n.samples
all_results=all_results[0,]

n.samples=c(12,24,36,48,60)
#n.samples=c(12,24,36,48)
for (iSample in 1:length(n.samples))
{
  RESULTS_PATH <- paste0("data/simResults-",n.samples[iSample],"-20230821.RData")
  load(RESULTS_PATH)
  all_results=tibble::add_row(
      all_results,
      error_factor = results$error_factor,
      method=results$method,
      lower=results$lower,
      point=results$point,
      upper=results$upper,
      point_runtime=results$point_runtime,
      conf_int_runtime=results$conf_int_runtime,
      B.lower=results$B.lower,
      B.point=results$B.point,
      B.upper=results$B.upper,
      n.samples=n.samples[iSample]
  )
}
```

```{r}
all_results %>%
  group_by(method, error_factor, n.samples) %>%
  summarise(point.pct_na = mean(point,na.rm=TRUE),
            lower.pct_na = mean(lower,na.rm=TRUE),
            upper.pct_na = mean(upper,na.rm=TRUE))
```

# Point Estimates

#### Calculate Metrics

```{r}
performance.point <- all_results %>%
  filter(!is.na(point)) %>% filter(error_factor == error_fac_to_plot) %>%
  group_by(method, n.samples) %>%
  summarise(MSE_000 = mean((point - theta.true)^2,na.rm=TRUE)/1000,
            bias = mean(point,na.rm=TRUE)-theta.true,
            variance_000 = var(point,na.rm=TRUE)/1000,
            avg_runtime = round(mean(point_runtime,na.rm=TRUE), 5))
```

```{r}
performance.point.tbl = vector(mode = "list", length(n.samples))

for (i in 1:length(n.samples)) {
  performance.point.tbl[[i]] <- performance.point %>%
    filter(n.samples == n.samples[i]) %>%
    ungroup() %>%
    mutate(across(!c(method, n.samples, avg_runtime), round)) %>%
    mutate(avg_runtime = round(avg_runtime, digits = 5)) %>%
    arrange(MSE_000)
}

performance.point.tbl[[1]]
performance.point.tbl[[2]]
performance.point.tbl[[3]]

```
Ignore run-times - I used that to store SEs for UN


#### Pivot to make plots and filter to `error_factor`

```{r}
performance.point.long <- performance.point %>%
  rename(Method = method, n.samples = n.samples, Bias = bias, Var_000 = variance_000, Runtime = avg_runtime) %>%
  pivot_longer(cols=c(MSE_000, Bias, Var_000, Runtime), names_to = "Metric")
  
performance.point.long
```

### Plots

```{r}
metrics = unique(performance.point.long$Metric)
performance.point_estimates.plots = lapply(metrics,
  function(met) {
    p = ggplot(data = filter(performance.point.long, Metric == met),
               mapping = aes(x = n.samples, y = value, colour = reorder(Method, value, decreasing=T))) +
      geom_line() +
      geom_point() +
      theme_bw() +
      labs(title = paste(met, "by Error"), ylab=NULL, colour = "Method") +
      theme(rect = element_rect(fill = "transparent")) +
      scale_color_manual(values = c("MINMI" = "#00BA38",
                                    "MLE" = "#619CFF",
                                    "BA-MLE" = "purple",
                                    "Strauss" = "orange",
                                    "GRIWM-corrected" = "darkgray",
                                    "GRIWM" = "maroon",
                                    "reginvUNci" = "darkblue",
                                    "reginvUNwald" = "red",
                                    "mlereginv" = "purple",
                                    "reginvUTci" = "lightblue",
                                    "reginvUTwald" = "pink",
                                    "mlereginvUT" = "plum",
                                    "mleInv2" = "plum",
                                    "mleInvP"="orchid",
                                    "mleInvAW" = "pink"))
    
    if (met %in% c("MSE", "Runtime")) {
      p = p+scale_y_log10(labels = label_comma())
    }
    p
  }
)

performance.point_estimates.plots[[1]] = performance.point_estimates.plots[[1]] + ylab("000's")
performance.point_estimates.plots[[2]] = performance.point_estimates.plots[[2]] + ylab("Years")
performance.point_estimates.plots[[3]] = performance.point_estimates.plots[[3]] + ylab("000's")

performance.point_estimates.plots[[1]]
performance.point_estimates.plots[[2]]
performance.point_estimates.plots[[3]]
```


## Commentary

1. MSE:
    1. MINMI has much higher MSE than UN-MLE
    2. MINMI MSE does not decrease as sample size increases (for large n) 
2. Bias:
    1. MINMI is more biased than UN (mean vs median thing I guess)
3. Variance:
    1. MINMI estimates generally have more variance than UN, and not decreasing with sample size (for large n)

```{r fig.width=15}
performance.point_estimates.plot.grid = do.call(grid.arrange, performance.point_estimates.plots)
performance.point_estimates.plot.grid
```


# Confidence Intervals

#### Calculate Metrics and Pivot

```{r}
performance.CI <- all_results %>%
  filter(!is.na(conf_int_runtime)) %>% filter(error_factor == error_fac_to_plot) %>%
  mutate(width = upper - lower,
         contains_theta = ifelse(theta.true > lower & theta.true < upper, 1, 0)) %>%
  group_by(n.samples, method) %>%
  summarise(Coverage = round(mean(contains_theta, na.rm=TRUE) * 100, 1),
            `Average Width` = round(mean(width, na.rm=TRUE), 2),
            `Average Runtime` = round(mean(conf_int_runtime, na.rm=TRUE), 5)) %>%
  ungroup() %>%
  arrange(method, n.samples)

performance.CI.long <- performance.CI %>%
  rename(n.samples = n.samples, Method = method, Width = `Average Width`, Runtime = `Average Runtime`) %>%
  pivot_longer(cols=c(Coverage, Width, Runtime),
               names_to = "Metric")
  
performance.CI.long
```

## Coverage Probability

```{r}
conf_int.coverage.plot <- performance.CI.long %>%
  filter(Metric == "Coverage") %>%
  ggplot(aes(x=n.samples, y=value, colour=reorder(Method, value, decreasing=T))) +
  geom_point() +
  geom_line(linewidth=0.5) +
  geom_label_repel(aes(label = value)) +
  theme_bw() +
  labs(y = "Coverage probability", colour="Method", title="Coverage Probabilities") +
  scale_y_continuous(breaks=c(0, 25, 50, 75, 95, 100)) +
  theme(rect = element_rect(fill = "transparent")) +
  scale_color_manual(values = c("GRIWM" = "#F8766D", "GRIWM-corrected" = "#619CFF", "MINMI" = "#00BA38", 
                                "reginvUNci" = "darkblue",
                                    "reginvUNwald" = "red",
                                    "mlereginv" = "purple",
                                    "reginvUTci" = "lightblue",
                                    "reginvUTwald" = "pink",
                                    "mlereginvUT" = "plum",
                                    "mleInv2" = "plum",
                                    "mleInvP"="orchid",
                                    "mleInvAW" = "pink"))
    

conf_int.coverage.plot
```

## Widths

```{r}
conf_int.width.plot <- performance.CI.long %>%
  filter(Metric == "Width") %>%
  ggplot(aes(x=n.samples, y=value, colour=reorder(Method, value, decreasing=T))) +
  geom_point() +
  geom_line(linewidth=0.5) +
  theme_bw() +
  labs(y = "Years", colour="Method", title="Average Width of Estimated Confidence Intervals") +
  theme(rect = element_rect(fill = "transparent")) +
  scale_color_manual(values = c("GRIWM" = "#F8766D", "GRIWM-corrected" = "#619CFF", "MINMI" = "#00BA38", 
                                "reginvUNci" = "darkblue",
                                    "reginvUNwald" = "red",
                                    "mlereginv" = "purple",
                                    "reginvUTci" = "lightblue",
                                    "reginvUTwald" = "pink",
                                    "mlereginvUT" = "plum",
                                    "mleInv2" = "plum",
                                    "mleInvP"="orchid",
                                    "mleInvAW" = "pink"))
    

conf_int.width.plot
```

## Runtime

```{r}
conf_int.runtime.plot <- performance.CI.long %>%
  filter(Metric == "Runtime") %>%
  ggplot(aes(x=n.samples, y=value, colour=reorder(Method, value, decreasing=T))) +
  geom_point() +
  geom_line(linewidth=0.5) +
  theme_bw() +
  scale_y_continuous(trans=scales::log10_trans()) +
  labs(y = "Seconds", colour="Method", title="Average Runtime of Confidence Interval Estimation") +
  theme(rect = element_rect(fill = "transparent")) +
  scale_color_manual(values = c("GRIWM" = "#F8766D", "GRIWM-corrected" = "#619CFF", "MINMI" = "#00BA38", 
                                "reginvUNci" = "darkblue",
                                    "reginvUNwald" = "red",
                                    "mlereginv" = "purple",
                                    "reginvUTci" = "lightblue",
                                    "reginvUTwald" = "pink",
                                    "mlereginvUT" = "plum",
                                    "mleInv2" = "plum",
                                    "mleInvP"="orchid",
                                    "mleInvAW" = "pink"))
    

conf_int.runtime.plot
```

## Commentary

1. Coverage Probability:
    1. MINMI generally has good coverage probability, as does MLE-INV
    2. Asymptotic UN-MLE methods have good coverage at moderate-large sample sizes (but not for small n)
    3. UN-Wald is slower to converge to desired coverage probability (symmetric CI)
2. Confidence Interval Widths:
    1. MINMI has way wider CIs
    2. MINMI CI width does not decrease as sample size increases
3. Runtime
    1. MLE_INV takes ages, MINMI a bit longer(!) Although I was using B=100, might have been fairer to compare at B=100 instead of doing choose B first

